# Creaci√≥n de un Agente de Inteligencia Artificial con LangGraph y OpenAI

| Title | Date |
| --- | --- |
| Creaci√≥n de un Agente de Inteligencia Artificial con LangGraph y OpenAI | 30/11/25 |

# Creaci√≥n de un Agente de Inteligencia Artificial con LangGraph y OpenAI

## **Contexto**

En esta actividad exploramos c√≥mo construir un agente b√°sico utilizando LangGraph, una herramienta dise√±ada para crear flujos conversacionales estructurados y controlados. A partir de un estado que mantiene el historial de mensajes, definimos nodos que representan comportamientos del agente y conectamos estos nodos mediante un grafo que determina el flujo de la conversaci√≥n. El objetivo es comprender c√≥mo LangGraph permite orquestar interacciones conversacionales de forma modular, transparente y extensible, sentando las bases para agentes m√°s avanzados con memoria, herramientas y l√≥gica personalizada.

## **Objetivo**

- Dise√±ar un estado de agente (AgentState) para conversaciones multi-turn.
- Construir un agente con LangGraph que:
    - use un modelo de chat (OpenAI) como reasoner,
    - llame tools externas (RAG + otra tool),
    - y mantenga el historial de conversaci√≥n.
- Integrar RAG como tool reutilizable (retriever + LLM).
- Agregar tools adicionales (p.ej. utilidades, "servicios" dummy).
- Orquestar LLM + tools en un grafo: assistant ‚Üî tools con bucles.
- Ejecutar conversaciones multi-turn y observar c√≥mo evoluciona el estado.

## **Actividades**

- **Parte 0: Setup y *Hello Agent* (LangGraph m√≠nimo)**
- **Parte 1: Estado del agente con memoria "ligera"**
- **Parte 2: Construir un RAG "mini" para usarlo como tool**
- **Parte 3: Otra tool adicional (no RAG)**
- **Parte 4: LLM con tool calling + ToolNode en LangGraph**
- **Parte 5: Conversaci√≥n multi-turn con el agente**
- **Parte 6: Interfaz Gradio para probar el agente**

## **Desarrollo**

Como en la actividad previa, antes de empezar, si el lector quiere, puede utilizar el archivo de Google Colab de las referencias para probar los modelos en su propia m√°quina. Para hacer esto deber√° tener su llave API de [LangSmith](https://smith.langchain.com/) y [OpenAI](https://platform.openai.com/api-keys). De igual manera, podr√° explorar este informe y ver los resultados obtenidos.

Para comenzar definimos la estructura fundamental sobre la que operar√° nuestro agente dentro de LangGraph: AgentState (estado del agente). Este ser√° representado mediante un TypedDict (o diccionario tipado). Este estado contiene una lista de mensajes que funciona como historial conversacional del agente, donde, cada vez que se generen nuevos mensajes estos ser√°n guardados en el estado acumulado. Esto permite mantener un seguimiento completo de la interacci√≥n, lo que le permite al modelo tener toda la informaci√≥n previa a la hora de recibir un mensaje nuevo, simulando como funciona una conversaci√≥n natural entre dos personas.

A continuaci√≥n definimos el primer nodo del grafo: assistant_node. Un nodo en LangGraph representa un paso de procesamiento dentro del flujo del agente, y en este caso su responsabilidad es simple pero central: llamar al modelo de lenguaje pasando el historial completo del estado. El nodo devuelve un diccionario con los nuevos mensajes, que ser√°n agregados al estado siguiendo la regla de combinaci√≥n definida anteriormente. As√≠ el agente produce una respuesta con sentido, bas√°ndose en todo lo que se ha dicho previamente.

Con esto podemos construir el grafo de estados utilizando StateGraph, agreg√°ndo el nodo asistente y definimos un flujo m√≠nimo: desde START hacia el nodo asistente y luego hacia el END. Este grafo lineal va a ejecutar el nodo una √∫nica vez, pero ya es suficiente para capturar la esencia del modelo mental de LangGraph, que es de representar agentes como transiciones expl√≠citas entre nodos que manipulan estados. ([Evidencia 1](#evidencia-1)).

Luego introducimos un nuevo concepto, el del estado compartido dentro de un agente. En lugar de procesar cada interacci√≥n de forma aislada, el agente mantiene ciertos elementos persistentes (como los mensajes intercambiados y un posible resumen) que le permiten comprender de mejor manera el contexto y evoluci√≥n o progreso de la conversaci√≥n. Este enfoque nos sirve para que el agente tenga una conducta m√°s coherente, sin olvidarse de qu√© viene hablando el usuario y qu√© contest√≥ √©l mismo, pudiendo condensarlo cuando sea necesario.

Adem√°s, podemos hacer uso de un corpus m√≠nimo y transformarlo en piezas manejables (chunks) para que luego el agente las use cuando las necesite. Haciendo esto tenemos un agente que combina memoria, contexto y acceso a informaci√≥n externa, lo que nos da un asistente m√°s inteligente, no solo capaz de responder, sino tambi√©n de recordar, resumir y apoyarse en informaci√≥n adicional cuando lo requiera.

Podemos hacer uso de herramientas para que el agente pueda realizar tareas de mejor manera, como puede ser la busca de informaci√≥n relevante dentro de un conjunto de elementos. La idea es que el agente no dependa solo de su modelo de lenguaje, sino que tenga acceso a un mecanismo externo que recupera datos reales desde un √≠ndice de vectores. El agente recibe un prompt y si ve que tiene que usar esta herramienta, la instancia y le pasa la pregunta, realizando una b√∫squeda sem√°ntica y devolviendo los fragmentos de texto m√°s cercanos al contenido consultado. ([Evidencia 2](#evidencia-2)). Podemos hacer esto con cualquier herramienta que nos sea √∫til para mejorar el rendimiento y capacidad del agente. ([Evidencia 3](#evidencia-3)).

Con las herramientas disponibles, ahora el agente puede tomar la decisi√≥n por s√≠ mismo de si responde directo o si llamar a las herramientas dar√° un mejor resultado (mejor calidad, menor tiempo o menor costo). ([Evidencia 4](#evidencia-4)).

A continuaci√≥n podemos ver algunas preguntas y respuestas a nuestro nuevo agente, armado con las herramientas vistas. ([Evidencia 5](#evidencia-5)). Podemos ver como el agente da una respuesta coherente seg√∫n el contexto que le dimos, siguiendo el sentido del primer texto. Tambi√©n podemos explicitarle al agente c√≥mo nos debe contestar. ([Evidencia 6](#evidencia-6)). A su vez podemos revisar c√≥mo son almacenados los mensajes para darle contexto al agente. ([Evidencia 7](#evidencia-7)).

Finalmente podemos levantar una interfaz para conversar con el agente, teniendo un producto funcional m√°s similar a lo que estamos acostumbrados hoy en d√≠a con ChatGPT o Gemini en la web. ([Evidencia 8](#evidencia-8)).

## **Evidencias**

### **Evidencia 1** {#evidencia-1}

![image.png](../assets/Agente/image.png)

```
Prompt:
	"Probando mi primer agente LangGraph :)"
Respuesta:
	¬°Genial ‚Äî felicidades por el primer agente! üôÇ ¬øQu√© te gustar√≠a probar ahora o en qu√© necesitas ayuda?

Puedo ayudarte con varias cosas, por ejemplo:
- Ejecutar una conversaci√≥n de prueba / simular interacciones.
- Generar prompts de prueba o ejemplos de intents/respuestas.
- Revisar la configuraci√≥n del agente (acciones, conectores, credenciales).
- Depurar errores (logs, permisos, respuestas inesperadas).
- Proporcionar un snippet de c√≥digo para llamar al agente desde tu app.

Dime qu√© prefieres: ¬øquieres que simule una conversaci√≥n ahora, que revise alguna configuraci√≥n concreta, o que te d√© un ejemplo de integraci√≥n?
```

### **Evidencia 2** {#evidencia-2}

![image.png](../assets/Agente/image-1.png)

### **Evidencia 3** {#evidencia-3}

![image.png](../assets/Agente/image-2.png)

### **Evidencia 4** {#evidencia-4}

![image.png](../assets/Agente/image-3.png)

### **Evidencia 5** {#evidencia-5}

![image.png](../assets/Agente/image-4.png)

```
Prompt:
	Hola, ¬øqu√© es LangGraph en pocas palabras?
Respuesta:
	LangGraph es una herramienta/plataforma para dise√±ar, ejecutar y depurar aplicaciones basadas en modelos de lenguaje usando diagramas de flujo (grafos). En pocas palabras: te permite conectar nodos (modelos, prompts, herramientas, pasos de l√≥gica) visualmente, probar y monitorear las ejecuciones, y luego exportar o ejecutar ese flujo como c√≥digo/servicio. Su objetivo es acelerar el desarrollo, facilitar la colaboraci√≥n y mejorar la trazabilidad y depuraci√≥n de pipelines LLM.  

	¬øQuieres que te muestre un ejemplo r√°pido o sus principales caracter√≠sticas?
```

### **Evidencia 6** {#evidencia-6}

```
Prompt:
	"Us√° tu base de conocimiento y decime qu√© es RAG."
Respuesta:
	RAG = Retrieval‚ÄëAugmented Generation. En pocas palabras: es una t√©cnica que combina recuperaci√≥n de informaci√≥n (search) con generaci√≥n de texto por un modelo de lenguaje para producir respuestas m√°s precisas y con respaldo documental.
	
	Resumen pr√°ctico
	- Qu√© hace: antes de generar, el sistema recupera documentos relevantes de una base de conocimientos; luego el LLM usa esos documentos como contexto para generar la respuesta.
	- Objetivo: reducir alucinaciones y permitir respuestas actuales/espec√≠ficas sin necesidad de fine‚Äëtuning completo del modelo.
	
	Componentes principales
	1. Retriever (recuperador): busca documentos relevantes en una colecci√≥n. Puede ser:
	 - Denso (embeddings + b√∫squeda vectorial).
	 - Disperso (BM25, b√∫squeda por palabras).
	2. Document store / Vector store: donde est√°n los textos indexados (chunking, metadatos, embeddings).
	3. Reranker (opcional): reordena o filtra los candidatos recuperados.
	4. Generator (LLM): toma la consulta + los documentos recuperados como contexto y genera la respuesta.
	5. Pipeline de mezcla: c√≥mo se incorpora la evidencia al prompt (concatenaci√≥n simple, prompting con few‚Äëshot, m√©todos RAG‚ÄëSequence/RAG‚ÄëToken).
	
	Variantes t√©cnicas relevantes
	- RAG‚ÄëSequence: el modelo genera usando los documentos ya seleccionados como contexto fijo.
	- RAG‚ÄëToken: el modelo puede considerar diferentes documentos en cada paso de generaci√≥n (m√°s complejo).
	- Retrieve‚Äëthen‚Äëread vs retrieve‚Äëand‚Äëgenerate: distintos flujos de c√≥mo se combinan recuperaci√≥n y generaci√≥n.
	
	Ventajas
	- Respuestas m√°s concretas y verificables.
	- Actualizaci√≥n de conocimiento sin reentrenar el LLM (solo actualizar la base de datos/embeddings).
	- Posibilidad de citar fuentes y mostrar evidencia.
	
	Limitaciones y riesgos
	- Si los documentos recuperados son irrelevantes o err√≥neos, la salida puede seguir siendo incorrecta.
	- Dependencia de la calidad del chunking y del motor de b√∫squeda.
	- Coste adicional por consultas y por tokens de contexto (si el contexto es grande).
	- Gesti√≥n de fuentes y confidencialidad si se usan datos sensibles.
	
	Buenas pr√°cticas
	- Indexar textos en fragmentos (chunking) con cuidado del tama√±o.
	- Usar re‚Äëranking y filtrado para mejorar precisi√≥n de los top‚Äëk recuperados.
	- Incluir metadatos y citas en la respuesta para trazabilidad.
	- Limitar y controlar la longitud del contexto para no saturar la ventana del LLM.
	- Evaluar con m√©tricas de relevancia y factualidad (Precision@k, exactitud en respuestas, tasa de citaci√≥n correcta).
	
	Casos de uso t√≠picos
	- Asistentes que responden con documentaci√≥n interna (FAQ, manuales, pol√≠ticas).
	- B√∫squeda conversacional y sistemas de soporte al cliente.
	- Generaci√≥n de res√∫menes de documentos largos usando pasajes relevantes.
	- Sistemas que necesitan conocimientos actualizados (noticias, bases legales).
	
	¬øQuieres un ejemplo concreto (arquitectura, pseudoc√≥digo o un mini‚Äëpipeline con OpenAI/embeddings + vector store)?
```

### **Evidencia 7** {#evidencia-7}

```
√öltimo mensaje: human ‚Üí Us√° tu base de conocimiento y decime qu√© es RAG.
√öltimo mensaje: ai ‚Üí RAG = Retrieval-Augmented Generation. En pocas palabras: es una t√©cnica que combina b√∫squeda (recuperaci√≥n) sobre una base de conocimiento con generaci√≥n por un modelo de lenguaje para producir respuestas m√°s informadas y con acceso a datos que el LLM no memoriza.

Concepto y c√≥mo funciona (resumen):
- Paso 1 ‚Äî Recuperaci√≥n: se toma la pregunta/consulta del usuario y se busca documentaci√≥n relevante en una base de datos (texto plano, documentos, embeddings en un vector DB, etc.).  
- Paso 2 ‚Äî Enriquecimiento: los fragmentos recuperados se pasan como contexto (o "evidence") al modelo de lenguaje.  
- Paso 3 ‚Äî Generaci√≥n: el LLM genera la respuesta condicionada en la consulta y en los fragmentos recuperados.

Componentes comunes:
- Indexador: divide y indexa documentos (chunking, embeddings).  
- Retriever: vector search / BM25 / re-ranker que trae los pasajes m√°s relevantes.  
- LLM (generador): produce la respuesta usando los pasajes como contexto.  
- Mecanismos de fusi√≥n/re-rank: a veces se reordena o se combina evidencia antes de generar.

Variantes t√©cnicas:
- RAG-Sequence / RAG-Token (m√©todos originales de Facebook) y enfoques como Fusion-in-Decoder (FiD): difieren en c√≥mo se pasan y combinan los pasajes al decoder del modelo.
- Recuperaci√≥n expl√≠cita + prompting (p. ej. "give answer and cite sources") es una pr√°ctica habitual.

Ventajas:
- Respuestas m√°s actualizadas y espec√≠ficas (no dependen solo del conocimiento interno del LLM).  
- Menor tama√±o del modelo necesario para acceder a gran volumen de informaci√≥n.  
- Mejora la factualidad cuando la recuperaci√≥n es buena.

Limitaciones y riesgos:
- Si el retriever trae pasajes irrelevantes o err√≥neos, el LLM puede mezclarlos y "alucinar" hechos.  
- Latencia mayor (consulta al √≠ndice + generaci√≥n).  
- Necesidad de mantener el √≠ndice actualizado y de tener buena calidad de embeddings/re-ranker.  
- Gesti√≥n de contexto: hay l√≠mites de tokens y hay que seleccionar/recortar pasajes.

Buenas pr√°cticas:
- Buena segmentaci√≥n y embeddings de los documentos.  
- Re-rankeos y filtros (por fecha, fuente, metadata).  
- Prompting que indique al LLM citar fuentes o decir "no s√©" si no hay evidencia.  
- Evaluar la precisi√≥n factual y medir el rendimiento con ejemplos reales.

Casos de uso t√≠picos:
- Asistentes empresariales con acceso a documentaci√≥n interna.  
- Soporte al cliente con base de conocimientos.  
- Sistemas de b√∫squeda conversacional y generaci√≥n de res√∫menes sobre documentos.

Si quer√©s, te muestro un diagrama simple del flujo o un ejemplo concreto de prompt + formato de pasajes recuperados. ¬øQuer√©s eso?
```

### **Evidencia 8** {#evidencia-8}

![image.png](../assets/Agente/image-5.png)

![image.png](../assets/Agente/image-6.png)

![image.png](../assets/Agente/image-7.png)

![image.png](../assets/Agente/image-8.png)

![image.png](../assets/Agente/image-9.png)

## **Reflexi√≥n**

En esta actividad pudimos armar un ambiente donde fuimos armando un agente de Inteligencia Artificial, haciendo uso de LangGraph, bas√°ndonos en el modelo del GPT-5-mini de OpenAI. Vimos qu√© es el contexto y las herramientas para un agente, c√≥mo las usa y c√≥mo almacena dicho contexto. Pudimos explorar con el armado de un corpus m√≠nimo para que el modelo pueda generar mejores respuestas sobre ciertos temas espec√≠ficos. Finalmente pudimos levantar una interfaz donde podemos intercambiar mensajes con el agente, pregunt√°ndole cosas y explicitando la manera en la que queremos que razone.

## **Referencias**

*Assignment UT4-14: Agentes con LangGraph ‚Äî RAG, Tools y Memoria Conversacional - Fundamentos del Aprendizaje Autom√°tico - Universidad Cat√≥lica del Uruguay*. (n.d.). [https://juanfkurucz.com/ucu-ia/ut4/15-agents/#parte-6-memoria-ligera-summary-como-nodo-extra-opcional](https://juanfkurucz.com/ucu-ia/ut4/15-agents/#parte-6-memoria-ligera-summary-como-nodo-extra-opcional)

*LangGraph*. (n.d.). [https://www.langchain.com/langgraph](https://www.langchain.com/langgraph)

*gpt-5-mini-2025-08-07*. (n.d.). [https://platform.openai.com/docs/models/gpt-5-mini](https://platform.openai.com/docs/models/gpt-5-mini)

*Google Colab*. (n.d.-m). [https://colab.research.google.com/drive/1oACxW94vEqx5KhUXWnwqAlQICWzM7pek?usp=sharing](https://colab.research.google.com/drive/1oACxW94vEqx5KhUXWnwqAlQICWzM7pek?usp=sharing)

